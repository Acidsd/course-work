%%
%% Author: natasha
%% 22.05.18
%%

% Preamble
\documentclass[10pt,a4paper]{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
%\usepackage{calrsfs}

\usepackage[utf8]{inputenc}
\usepackage{graphicx} % Allows including images
\usepackage[final]{hyperref}
\usepackage{cite}
\usepackage[margin=0.5in,a4paper]{geometry}

\author{Pavlovskaia Natalia}
\title{Course work report}

% Document
\begin{document}
    \maketitle

    \section{Results at the end of the Term 3}\label{sec:resultsAtTheEndOfTheTerm3}

    At the end of the Term 3 the paper~\cite{original_paper} was reproduced for the UKB dataset.
    The result clearly showed overfitting which was in agreement with the original paper.

    It was decided to try the following things:
    \begin{itemize}
        \item {generate larger dataset using the Omniglot with several labels per image}
        \item {classify the positive and negative pairs in this new dataset using binary classification}
        \item {use Poincare embeddings for this new dataset}
        \item {investigate the overfitting problem for UKB dataset}
    \end{itemize}

    \section{Results at the end of the Term 4}\label{sec:results}

    What was actually done

    \begin{itemize}
        \item {generate larger dataset using the Omniglot with several labels per image - \textbf{done}}
        \item {classify the positive and negative pairs in this new dataset using binary classification - \textbf{done,
        results are not good, described below}}
        \item {use Poincare embeddings for this new dataset - \textbf{is not done, just started}}
        \item {investigate the overfitting problem for UKB dataset - \textbf{some progress is achieved,
        results are more or less hopeful, described below}}
    \end{itemize}

    Results are presented in the table~\nameref{table:main_table}.
    The code is available here \url{https://github.com/ne-bo/course-work}.
    \newline

    I tried out different \underline{sampling strategies} and \underline{regularization techniques}.
    \newline

    Regarding the \underline{sampling strategies}. In my mind the dataset is a big matrix. And each batch is a small submatrix.
    Each element of the matrix corresponds to the pair of $i$-th and $j-th$ images. This pair can be positive or negative.
    For the UKB dataset the first 4 rows (columns) of the big matrix corresponds to the first class, the second 4 rows (columns)
    corresponds to the second class and so on up to the 1275-th 4 rows (columns) corresponds to the class number 1275.
    \newline

    During the training we can iterate over batches.
    \newline

    The sampling strategy \textit{all} means that we use all small submatrices of our big matrix. In this case we use
    much more negative pairs than positive. But I think this is anyway good for the step 1 (just learning cosine similarity).
    From experiments I see that for 2 step it is also good.
    \newline

    The sampling strategy \textit{diagonal} means that we use all only small submatrices on the main diagonal of our big matrix.
    These batches contain all our positive pairs and some negative pairs. Again we have more negative pairs than positive.
    But we don't use all possiblenegative examples. This strategy makes training faster and somehow balance positive and negative pairs.
    \newline

    The sampling strategy \textit{equal} means that we use all only small submatrices on the main diagonal of our big matrix while iterating.
    We compute the forward path for all pairs in the batch. But when we compute the loss in the end we take into account all positive pairs
    and the same number of random negative pairs taken from our batch. This strategy allows us to have perfectly balanced positive and negative pairs.
    But experiments shows that we don't gain anything using this strategy.
    \newline

    For the first step I provide the strategy name.

    For the second step I provide the name for the strategy of the 1 step + the name for the strategy of the 2 step.
    I've tried out different combinations.
    \newline

    Regarding the \underline{regularization techniques}. I realized that we have overfitting after the 1 step.
    So I've tried to regularize somehow our small net for the metric learning.
    \newline

    The regularization \textit{1024 instead of 2048} means that we simply use 2 layers of 1024 hidden neurons instead
    of 2 layers of 2048 hidden neurons. As I know decreasing the capacity of the model can help.
    \newline

    The regularization \textit{dropout} means that we insert a dropout layer with $p=0.5$ between our 2 hidden layers.
    The best result so far is achieved with this type of regularization and marked in \textbf{bold} in the table.
    \newline

    \begin{table}[h!]
        \begin{tabular}{|l|l|l|l|l|l|l|}
            \hline
            Dataset & Conv & k & Average recall at k & Average recall at k & Sampling & Regularization       \\
            & part & & Train & Test & strategy &                      \\ \hline
            %--------------------------------------------------------    -     --------------------    ---     ------        -------------------------
            & & & 0.909657 raw SPoC & 0.839461 raw SPoC & &                      \\ \cline{4-7}

            & & & 0.872304 1 step & 0.590588 1 step & diagonal &                      \\
            & & & 0.781176 1 step & 0.486127 1 step & equal &                      \\
            & & & 0.835294 1 step & 0.588725 1 step & all &                      \\
            & & & 0.765196 1 step & 0.535980 1 step & all & 1024 instead of 2048 \\
            & & & 0.464510 1 step & 0.405637 1 step & all & dropout              \\ \cline{4-7}

            UKB & VGG & 4 & 0.915147 2 step (delta)     & 0.622059 2 step (delta)     & diagonal + diagonal &                      \\
            & & & 0.869118 2 step (delta)     & 0.535735 2 step (delta)     & diagonal + equal &                      \\
            & & & 0.828971 2 step (delta)     & 0.491765 2 step (delta)     & equal + equal &                      \\
            & & & 0.926225 2 step (delta)     & 0.659118 2 step (delta)     & all + all &                      \\
            & & & 0.918971 2 step (delta)     & 0.644657 2 step (delta)     & all + all & 1024 instead of 2048 \\
            & & & \textbf{0.938922} 2 step (delta) & \textbf{0.718235} 2 step (delta) & all + all & dropout              \\
            & & & 0.840392 2 step (delta)     & 0.624216 2 step (delta)     & all + diagonal & dropout              \\ \cline{4-7}

            & & & ??? 2 step (histogram)      & ??? 2 step (histogram)      & &                      \\
            & & & ??? 2 step (margin)         & ??? 2 step (margin)         & &                      \\ \hline \hline
            %-----------------------------------------------------     ---------------------------     -------------------------------
            %             &            &   & raw SPoC                    & raw SPoC                    &                     &                      \\
            %             &            &   & 1 step                      & 1 step                      &                     &                      \\
            %    Omniglot & Alexnet    & ? & 2 step (delta)              & 2 step (delta)              &                     &                      \\
            %             &            &   & 2 step (histogram)          & 2 step (histogram)          &                     &                      \\
            %             &            &   & 2 step (margin)             & 2 step (margin)             &                     &                      \\ \hline
            %-----------------------------------------------------         -----------------------    ----     -------------------------------
            %    Omniglot & Alexnet    & ? & Poincare                    & Poincare                    &                     &                      \\ \hline \hline
            Dataset & Conv & & f1 score & f1 score & Sampling & Regularization       \\
            & part & & Train & Test & strategy &                      \\ \hline
            Omniglot & Alexnet & & ??? binary classification & ??? binary classification & equal &                      \\
            \hline
        \end{tabular}
        \caption{Results}
        \label{table:main_table}
    \end{table}

    \subsection{Comments}\label{subsec:comments}

    The results for the histogram and margin loss are not presented here because they are very poor.
    I suppose that for histogram loss we just don't have enough positive pairs. Each batch contains at most 246 of them.
    For margin loss I don't have a good hypotesis.
    The results for classification are also very poor.
    \newline

    I had calculated them in the past and don't remember exact numbers. Recalculation requires time.
    I'll fill all missing rows later.
    \newline

    My future plan:
    \begin{itemize}
        \item {Play more with regularization for UKB. From my point of view this is the most promising thing.}
        \item {Deal with Omniglot dataset not in classification mode but in the same fashion as with UKB. It is promising because of the bigger size
        of Omniglot dataset.}
        \item {Try classification with the regularization.}
        \item {Move further with Poincare embeddings. For now I've adopted the existing code, but not fully.}
    \end{itemize}



    \section{Technical details}\label{sec:technicalDetails}

    \textbf{Quality metric for UKB:}
    \begin{itemize}
        \item{$N = $ total number of examples}
        \item{$C_i^k = $ number of correct answers for the $i$-th example among the top $k$}
        \item{$C_i^t = $ total number of correct answers for the $i$-th example among the all examples}
    \end{itemize}

    \[\text{average recall at k} = \frac{1}{N}\sum_{i = 1}^{N}
    \frac{C_i^k}{C_i^t}\]

    \textbf{Technical details:}
    \begin{itemize}
        \item{batch size -

        $170 \times 170$ for UKB,

        $270 \times 270$ for Omniglot}
        \item{loss function -

        nn.MSELoss() for the 1 step,

        nn.MSELoss() or MarginLossForSimilarity() or HistogramLossForSimilarity() for the 2 step,

        nn.CrossEntropyLoss() for binary classification}
        \item{learning rate -

        0.001 and its decay coefficient 0.8, learning rate is decaying every 10 epochs for UKB,

        0.01 and its its decay coefficient 0.1 for Omniglot}
        \item{optimizer - Adam}
        \item{number of epochs - 100}
    \end{itemize}

    \subsection{Datasets description}\label{subsec:datasetsDescription}

    \begin{table}[h!]
        \begin{tabular}{|l|l|l|r|l|l|}
            \hline
            Name & Train size & Test size & Total Number & Number of images & Comments                  \\
            & & & of classes & in each class &                           \\ \hline
            %-----------------------------------------------------------------------------------------------------------------------
            UKB & 5100 & 5100 & 1275 in train, 1275 in test & 4 & Spocs are calculated once \\ \hline
            Omniglot & 10000 & 10000 & 964 in train, \space659 in test & 20 & 1-3 symbols per image      \\ \hline
        \end{tabular}
        \caption{Datasets}
        \label{table:datasets_table}
    \end{table}


    %\includegraphics{}
    \begin{thebibliography}{9}

        \bibitem{original_paper}
        Noa Garsia, George Vogiatzis.
        \textit{Learning Non-Metric Visual Similarity for Image Retrieval}. 2017
        \url{https://arxiv.org/abs/1709.01353}

    \end{thebibliography}

\end{document}